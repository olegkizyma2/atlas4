2025-10-12 00:22:57,139 [ukrainian-tts-server] INFO: Python executable: /Users/dev/Documents/GitHub/atlas4/ukrainian-tts/.venv/bin/python3
2025-10-12 00:22:57,139 [ukrainian-tts-server] INFO: VIRTUAL_ENV/CONDA_PREFIX: /Users/dev/Documents/GitHub/atlas4/ukrainian-tts/.venv
2025-10-12 00:22:57,139 [ukrainian-tts-server] INFO: sys.path (first entries): ['/Users/dev/Documents/GitHub/atlas4/ukrainian-tts', '/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '/Users/dev/Documents/GitHub/atlas4/ukrainian-tts/.venv/lib/python3.11/site-packages', '/opt/homebrew/opt/python-tk@3.11/libexec']
2025-10-12 00:22:57,139 [ukrainian-tts-server] INFO: Initializing Ukrainian TTS on device: mps
/Users/dev/Documents/GitHub/atlas4/ukrainian-tts/.venv/lib/python3.11/site-packages/pyworld/__init__.py:13: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/Users/dev/Documents/GitHub/atlas4/ukrainian-tts/.venv/lib/python3.11/site-packages/torch/package/package_importer.py:257: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  dtype = storage_type.dtype
2025-10-12 00:22:59,250 [stanza] INFO: Loading these models for language: uk (Ukrainian):
=========================
| Processor | Package   |
-------------------------
| tokenize  | iu        |
| mwt       | iu        |
| pos       | iu_charlm |
=========================

2025-10-12 00:22:59,250 [stanza] INFO: Using device: cpu
2025-10-12 00:22:59,250 [stanza] INFO: Loading: tokenize
2025-10-12 00:22:59,700 [stanza] INFO: Loading: mwt
2025-10-12 00:22:59,703 [stanza] INFO: Loading: pos
2025-10-12 00:23:00,785 [stanza] INFO: Done loading processors!
2025-10-12 00:23:00,835 [root] INFO: Vocabulary size: 48
/Users/dev/Documents/GitHub/atlas4/ukrainian-tts/.venv/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
  WeightNorm.apply(module, name, dim)
2025-10-12 00:23:01,701 [root] INFO: Extractor:
LogMelFbank(
  (stft): Stft(n_fft=1024, win_length=1024, hop_length=256, center=True, normalized=False, onesided=True)
  (logmel): LogMel(sr=22050, n_fft=1024, n_mels=80, fmin=80, fmax=7600, htk=False)
)
2025-10-12 00:23:01,701 [root] INFO: Normalizer:
GlobalMVN(stats_file=feats_stats.npz, norm_means=True, norm_vars=True)
2025-10-12 00:23:01,702 [root] INFO: TTS:
JointText2Wav(
  (generator): ModuleDict(
    (text2mel): Tacotron2(
      (enc): Encoder(
        (embed): Embedding(48, 512, padding_idx=0)
        (convs): ModuleList(
          (0-2): 3 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU()
            (3): Dropout(p=0.5, inplace=False)
          )
        )
        (blstm): LSTM(512, 256, batch_first=True, bidirectional=True)
      )
      (projection): Linear(in_features=192, out_features=512, bias=True)
      (dec): Decoder(
        (att): AttLoc(
          (mlp_enc): Linear(in_features=512, out_features=512, bias=True)
          (mlp_dec): Linear(in_features=1024, out_features=512, bias=False)
          (mlp_att): Linear(in_features=32, out_features=512, bias=False)
          (loc_conv): Conv2d(1, 32, kernel_size=(1, 31), stride=(1, 1), padding=(0, 15), bias=False)
          (gvec): Linear(in_features=512, out_features=1, bias=True)
        )
        (lstm): ModuleList(
          (0): ZoneOutCell(
            (cell): LSTMCell(768, 1024)
          )
          (1): ZoneOutCell(
            (cell): LSTMCell(1024, 1024)
          )
        )
        (prenet): Prenet(
          (prenet): ModuleList(
            (0): Sequential(
              (0): Linear(in_features=80, out_features=256, bias=True)
              (1): ReLU()
            )
            (1): Sequential(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): ReLU()
            )
          )
        )
        (postnet): Postnet(
          (postnet): ModuleList(
            (0): Sequential(
              (0): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Tanh()
              (3): Dropout(p=0.5, inplace=False)
            )
            (1-3): 3 x Sequential(
              (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
              (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Tanh()
              (3): Dropout(p=0.5, inplace=False)
            )
            (4): Sequential(
              (0): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
              (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Dropout(p=0.5, inplace=False)
            )
          )
        )
        (feat_out): Linear(in_features=1536, out_features=80, bias=False)
        (prob_out): Linear(in_features=1536, out_features=1, bias=True)
      )
      (taco2_loss): Tacotron2Loss(
        (l1_criterion): L1Loss()
        (mse_criterion): MSELoss()
        (bce_criterion): BCEWithLogitsLoss()
      )
      (attn_loss): GuidedAttentionLoss()
    )
    (vocoder): HiFiGANGenerator(
      (input_conv): Conv1d(80, 512, kernel_size=(7,), stride=(1,), padding=(3,))
      (upsamples): ModuleList(
        (0): Sequential(
          (0): LeakyReLU(negative_slope=0.1)
          (1): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))
        )
        (1): Sequential(
          (0): LeakyReLU(negative_slope=0.1)
          (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))
        )
        (2): Sequential(
          (0): LeakyReLU(negative_slope=0.1)
          (1): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))
        )
        (3): Sequential(
          (0): LeakyReLU(negative_slope=0.1)
          (1): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))
        )
      )
      (blocks): ModuleList(
        (0): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
            )
          )
        )
        (1): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
            )
          )
        )
        (2): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
            )
          )
        )
        (3): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            )
          )
        )
        (4): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
            )
          )
        )
        (5): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
            )
          )
        )
        (6): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            )
          )
        )
        (7): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
            )
          )
        )
        (8): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
            )
          )
        )
        (9): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            )
          )
        )
        (10): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
            )
          )
        )
        (11): ResidualBlock(
          (convs1): ModuleList(
            (0): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
            )
            (1): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
            )
            (2): Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
            )
          )
          (convs2): ModuleList(
            (0-2): 3 x Sequential(
              (0): LeakyReLU(negative_slope=0.1)
              (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
            )
          )
        )
      )
      (output_conv): Sequential(
        (0): LeakyReLU(negative_slope=0.01)
        (1): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))
        (2): Tanh()
      )
    )
  )
  (discriminator): HiFiGANMultiScaleMultiPeriodDiscriminator(
    (msd): HiFiGANMultiScaleDiscriminator(
      (discriminators): ModuleList(
        (0-2): 3 x HiFiGANScaleDiscriminator(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(1, 128, kernel_size=(15,), stride=(1,), padding=(7,))
              (1): LeakyReLU(negative_slope=0.1)
            )
            (1): Sequential(
              (0): Conv1d(128, 128, kernel_size=(41,), stride=(4,), padding=(20,), groups=4)
              (1): LeakyReLU(negative_slope=0.1)
            )
            (2): Sequential(
              (0): Conv1d(128, 256, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)
              (1): LeakyReLU(negative_slope=0.1)
            )
            (3): Sequential(
              (0): Conv1d(256, 512, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)
              (1): LeakyReLU(negative_slope=0.1)
            )
            (4): Sequential(
              (0): Conv1d(512, 1024, kernel_size=(41,), stride=(4,), padding=(20,), groups=16)
              (1): LeakyReLU(negative_slope=0.1)
            )
            (5): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(41,), stride=(1,), padding=(20,), groups=16)
              (1): LeakyReLU(negative_slope=0.1)
            )
            (6): Sequential(
              (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))
              (1): LeakyReLU(negative_slope=0.1)
            )
            (7): Conv1d(1024, 1, kernel_size=(3,), stride=(1,), padding=(1,))
          )
        )
      )
      (pooling): AvgPool1d(kernel_size=(4,), stride=(2,), padding=(2,))
    )
    (mpd): HiFiGANMultiPeriodDiscriminator(
      (discriminators): ModuleList(
        (0-4): 5 x HiFiGANPeriodDiscriminator(
          (convs): ModuleList(
            (0): Sequential(
              (0): Conv2d(1, 32, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))
              (1): LeakyReLU(negative_slope=0.1)
            )
            (1): Sequential(
              (0): Conv2d(32, 128, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))
              (1): LeakyReLU(negative_slope=0.1)
            )
            (2): Sequential(
              (0): Conv2d(128, 512, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))
              (1): LeakyReLU(negative_slope=0.1)
            )
            (3): Sequential(
              (0): Conv2d(512, 1024, kernel_size=(5, 1), stride=(3, 1), padding=(2, 0))
              (1): LeakyReLU(negative_slope=0.1)
            )
            (4): Sequential(
              (0): Conv2d(1024, 1024, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))
              (1): LeakyReLU(negative_slope=0.1)
            )
          )
          (output_conv): Conv2d(1024, 1, kernel_size=(2, 1), stride=(1, 1), padding=(1, 0))
        )
      )
    )
  )
  (generator_adv_loss): GeneratorAdversarialLoss()
  (discriminator_adv_loss): DiscriminatorAdversarialLoss()
  (feat_match_loss): FeatureMatchLoss()
  (mel_loss): MelSpectrogramLoss(
    (wav_to_mel): LogMelFbank(
      (stft): Stft(n_fft=1024, win_length=1024, hop_length=256, center=True, normalized=False, onesided=True)
      (logmel): LogMel(sr=22050, n_fft=1024, n_mels=80, fmin=0, fmax=11025.0, htk=False)
    )
  )
)
2025-10-12 00:23:01,899 [ukrainian-tts-server] INFO: Ukrainian TTS initialized successfully
2025-10-12 00:23:01,900 [ukrainian-tts-server] INFO: Ukrainian TTS Server initialized at 127.0.0.1:3001
2025-10-12 00:23:01,900 [ukrainian-tts-server] INFO: Starting Ukrainian TTS Server on 127.0.0.1:3001
2025-10-12 00:23:01,900 [ukrainian-tts-server] INFO: TTS ready: True
2025-10-12 00:23:01,900 [ukrainian-tts-server] INFO: Device: mps
downloading https://github.com/robinhad/ukrainian-tts/releases/download/v6.0.0
Found ../model.pth. Skipping download...
Found ../config.yaml. Skipping download...
Found ../spk_xvector.ark. Skipping download...
Found ../feats_stats.npz. Skipping download...
downloaded.
 * Serving Flask app 'tts_server'
 * Debug mode: off
2025-10-12 00:23:01,905 [werkzeug] INFO: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:3001
2025-10-12 00:23:01,905 [werkzeug] INFO: [33mPress CTRL+C to quit[0m
2025-10-12 00:23:06,732 [werkzeug] INFO: 127.0.0.1 - - [12/Oct/2025 00:23:06] "OPTIONS /health HTTP/1.1" 200 -
2025-10-12 00:23:06,733 [werkzeug] INFO: 127.0.0.1 - - [12/Oct/2025 00:23:06] "GET /health HTTP/1.1" 200 -
2025-10-12 00:24:21,495 [werkzeug] INFO: 127.0.0.1 - - [12/Oct/2025 00:24:21] "OPTIONS /tts HTTP/1.1" 200 -
2025-10-12 00:24:21,498 [ukrainian-tts-server] INFO: TTS request: text='Ð¡Ñ…Ð¾Ð¶Ðµ, Ð²Ð¸ Ð½Ð°Ð²Ð¾Ð´Ð¸Ñ‚Ðµ ÑÐºÑƒÑÑŒ Ð¿Ð¾ÑÐ»Ñ–Ð´Ð¾Ð²Ð½Ñ–ÑÑ‚ÑŒ. Ð§Ð¸ Ñ” ÐºÐ¾Ð½ÐºÑ€...', voice=mykyta, fx=none, length=98 chars
/Users/dev/Documents/GitHub/atlas4/ukrainian-tts/.venv/lib/python3.11/site-packages/espnet2/torch_utils/device_funcs.py:29: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:209.)
  return to_device(torch.from_numpy(data), device, dtype, non_blocking, copy)
2025-10-12 00:24:24,254 [werkzeug] INFO: 127.0.0.1 - - [12/Oct/2025 00:24:24] "POST /tts HTTP/1.1" 200 -
