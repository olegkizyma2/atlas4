# ===================================
# ATLAS v5.0 - Environment Configuration
# ===================================
# Copy this file to .env and configure your settings
# Last Updated: 2025-10-16 (v5.0 - Pure MCP Edition with AI Model Config)

# === SYSTEM ===
NODE_ENV=production
BUILD_NUMBER=dev
LOG_LEVEL=info
FORCE_FREE_PORTS=true

# === LLM API CONFIGURATION (NEW v5.0) ===
# Primary LLM API endpoint (local)
LLM_API_ENDPOINT=http://localhost:4000/v1/chat/completions

# Fallback LLM API endpoint (ngrok tunnel for remote access)
# Example: https://bdd80b70a92d.ngrok-free.app/v1/chat/completions
LLM_API_FALLBACK_ENDPOINT=https://64dc06a712c1.ngrok-free.app/v1/chat/completions

# Enable fallback to remote API when local unavailable
LLM_API_USE_FALLBACK=true

# API timeout in milliseconds
LLM_API_TIMEOUT=60000

# === AI MODELS CONFIGURATION (UPDATED 16.10.2025) ===
# System stages models (stage0_mode_selection, stage0_chat, stage-2_post_chat_analysis, stage-3_tts_optimization)
# Default values are already optimized - only override if needed
# AI_MODEL_CLASSIFICATION: Default = mistral-ai/ministral-3b (45 req/min)
# AI_MODEL_CHAT: Default = mistral-ai/mistral-small-2503 (40 req/min, краща якість для розмови)
# AI_MODEL_ANALYSIS: Default = openai/gpt-4o-mini (35 req/min)
# AI_MODEL_TTS_OPT: Default = mistral-ai/ministral-3b (45 req/min)
# Example override:
# AI_MODEL_CLASSIFICATION=openai/gpt-4o-mini
# AI_MODEL_CHAT=meta/llama-3.3-70b-instruct
# AI_MODEL_ANALYSIS=cohere/cohere-command-r-plus-08-2024
# AI_MODEL_TTS_OPT=microsoft/phi-4-mini-instruct

# System stage temperatures (override defaults if needed)
# Default: AI_TEMP_CLASSIFICATION=0.05 (максимальна точність)
# Default: AI_TEMP_CHAT=0.7 (креативність)
# Default: AI_TEMP_ANALYSIS=0.2 (аналіз)
# Default: AI_TEMP_TTS_OPT=0.15 (стабільність)
# Example override:
# AI_TEMP_CLASSIFICATION=0.1
# AI_TEMP_CHAT=0.9

# === MCP MODELS CONFIGURATION (UPDATED 16.10.2025) ===
# Models for MCP stages (stage0_mode_selection, stage1_todo_planning, etc.)
# Default values are already optimized - only override if needed
#
# Stage 0: Mode Selection - Default = mistral-ai/ministral-3b (45 req/min)
# MCP_MODEL_MODE_SELECTION=mistral-ai/ministral-3b
# MCP_TEMP_MODE_SELECTION=0.05
#
# Stage 0.5: Backend Selection - Default = mistral-ai/ministral-3b (45 req/min, deprecated)
# MCP_MODEL_BACKEND_SELECTION=mistral-ai/ministral-3b
# MCP_TEMP_BACKEND_SELECTION=0.05
#
# Stage 1: Atlas TODO Planning - Default = mistral-ai/mistral-small-2503 (40 req/min)
# MCP_MODEL_TODO_PLANNING=mistral-ai/mistral-small-2503
# MCP_TEMP_TODO_PLANNING=0.3
#
# Stage 2.1: Tetyana Plan Tools - Default = mistral-ai/mistral-small-2503 (40 req/min)
# MCP_MODEL_PLAN_TOOLS=mistral-ai/mistral-small-2503
# MCP_TEMP_PLAN_TOOLS=0.1
#
# Stage 2.3: Grisha Verify Item - Default = mistral-ai/mistral-small-2503 (40 req/min)
# MCP_MODEL_VERIFY_ITEM=mistral-ai/mistral-small-2503
# MCP_TEMP_VERIFY_ITEM=0.15
#
# Stage 3: Atlas Adjust TODO - Default = openai/gpt-4o-mini (35 req/min)
# MCP_MODEL_ADJUST_TODO=openai/gpt-4o-mini
# MCP_TEMP_ADJUST_TODO=0.2
#
# Stage 8: Final Summary - Default = mistral-ai/ministral-3b (45 req/min)
# MCP_MODEL_FINAL_SUMMARY=mistral-ai/ministral-3b
# MCP_TEMP_FINAL_SUMMARY=0.5
#
# === RECOMMENDED MODEL ALTERNATIVES (за наявністю) ===
# High quality options (якість > швидкість):
#   - openai/gpt-4o-mini (35 req/min) - найкраща якість для гріші
#   - meta/llama-3.3-70b-instruct (4 req/min) - потужна, але повільна
#   - cohere/cohere-command-r-08-2024 (10 req/min) - добра якість
#
# Fast options (швидкість > якість):
#   - mistral-ai/ministral-3b (45 req/min) - найбільш доступна
#   - meta/llama-3.2-11b-vision-instruct (6 req/min) - швидка з vision
#   - microsoft/phi-3.5-mini-instruct (38 req/min) - добра швидкість
#
# Balanced options (якість = швидкість):
#   - mistral-ai/mistral-small-2503 (40 req/min) - оптимальний баланс
#   - microsoft/phi-4 (8 req/min) - потужна і швидка
#   - mistral-ai/mistral-medium-2505 (18 req/min) - середній рівень

# === AI BACKEND CONFIGURATION ===
# v5.0: Pure MCP mode only (Goose deprecated)
# Backend mode: 'mcp' (recommended) | 'goose' (deprecated) | 'hybrid' (deprecated)
AI_BACKEND_MODE=mcp

# Primary backend for task execution
AI_BACKEND_PRIMARY=mcp

# Disable fallback (strict mode for development/testing)
# - true: System will throw errors on failures (no fallback)
# - false: System will attempt fallback (production safe)
AI_BACKEND_DISABLE_FALLBACK=false

# === DEPRECATED: GOOSE DESKTOP (v4.0 Legacy) ===
# WARNING: Goose integration is deprecated in v5.0
# These settings are kept for backward compatibility only
# Will be removed in future versions
# GOOSE_BIN=/Applications/Goose.app/Contents/MacOS/goose
# GOOSE_DESKTOP_PATH=/Applications/Goose.app/Contents/MacOS/goose
# GOOSE_SERVER_PORT=3000
# GOOSE_PORT=3000
# GOOSE_DISABLE_KEYRING=1
# GITHUB_TOKEN=your_github_token_here

# === TTS & VOICE ===
REAL_TTS_MODE=true
TTS_DEVICE=mps
TTS_PORT=3001

# === WHISPER CONFIGURATION ===
# Optimized for Mac Studio M1 MAX (Metal GPU)
WHISPER_BACKEND=cpp
WHISPER_DEVICE=metal
WHISPER_PORT=3002
WHISPER_CPP_BIN=/Users/dev/Documents/GitHub/atlas4/third_party/whisper.cpp.upstream/build/bin/whisper-cli
WHISPER_CPP_MODEL=/Users/dev/Documents/GitHub/atlas4/models/whisper/ggml-large-v3.bin
# Mac Studio M1 MAX optimizations:
WHISPER_CPP_NGL=20              # GPU layers (Metal acceleration)
WHISPER_CPP_THREADS=10          # M1 Max has 10 performance cores
WHISPER_CPP_DISABLE_GPU=false   # Keep Metal GPU enabled
WHISPER_SAMPLE_RATE=48000       # High quality audio (v5.0)

# === NETWORK PORTS ===
ORCHESTRATOR_PORT=5101
WEB_PORT=5001
FRONTEND_PORT=5001

# === FEATURES ===
# Mac Studio M1 MAX optimizations
USE_METAL_GPU=true              # Metal GPU for Whisper and TTS
OPTIMIZE_FOR_M1_MAX=true        # Enable M1 MAX specific optimizations
